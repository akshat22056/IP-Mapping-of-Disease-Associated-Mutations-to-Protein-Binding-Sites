{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrJFtG1Lqnrw",
        "outputId": "718c412f-823f-49b6-8402-291b638a73b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 763 unique UniProt IDs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching features: 100%|██████████| 763/763 [00:15<00:00, 49.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying features to dataframe...\n",
            "\n",
            "Processed 787 rows in 0.26 minutes\n",
            "Results saved to output_with_sites.csv\n",
            "Found binding sites for 762 proteins\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import re\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Cache to store already fetched features\n",
        "UNIPROT_CACHE = {}\n",
        "SESSION = requests.Session()\n",
        "\n",
        "def get_sequence_from_feature(feature, full_sequence, start, end):\n",
        "    \"\"\"Extract the actual sequence from the protein sequence\"\"\"\n",
        "    try:\n",
        "        # First try to get from description\n",
        "        desc = feature.get('description', '')\n",
        "        if '(' in desc and ')' in desc:\n",
        "            match = re.search(r'\\(([A-Z]+)\\)', desc)\n",
        "            if match:\n",
        "                return match.group(1)\n",
        "\n",
        "        # If not in description, extract from full sequence\n",
        "        return full_sequence[start-1:end]  # -1 because Python is 0-indexed\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def get_uniprot_features(uniprot_id):\n",
        "    \"\"\"Get binding/active sites for a single UniProt ID\"\"\"\n",
        "    if pd.isna(uniprot_id) or uniprot_id == \"Not found\":\n",
        "        return []\n",
        "\n",
        "    if uniprot_id in UNIPROT_CACHE:\n",
        "        return UNIPROT_CACHE[uniprot_id]\n",
        "\n",
        "    url = f\"https://rest.uniprot.org/uniprotkb/{uniprot_id}.json\"\n",
        "\n",
        "    try:\n",
        "        with SESSION.get(url, timeout=30) as response:\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            sites = []\n",
        "            full_sequence = data.get('sequence', {}).get('value', '')\n",
        "\n",
        "            for feature in data.get('features', []):\n",
        "                if feature['type'] in ('Binding site', 'Active site'):\n",
        "                    try:\n",
        "                        location = feature['location']\n",
        "                        start = location['start']['value']\n",
        "                        end = location['end']['value']\n",
        "                        pos = f\"{start}\" if start == end else f\"{start}-{end}\"\n",
        "\n",
        "                        sequence = get_sequence_from_feature(feature, full_sequence, start, end)\n",
        "\n",
        "                        if sequence:\n",
        "                            sites.append((pos, sequence))\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "            UNIPROT_CACHE[uniprot_id] = sites\n",
        "            return sites\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {uniprot_id}: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def process_csv(input_file, output_file, max_sites=10, max_workers=8):\n",
        "    \"\"\"Process CSV and add position/sequence columns\"\"\"\n",
        "    # Read input CSV\n",
        "    df = pd.read_csv(input_file)\n",
        "\n",
        "    # Prepare empty columns\n",
        "    for i in range(1, max_sites+1):\n",
        "        df[f'Position_{i}'] = pd.NA\n",
        "        df[f'Sequence_{i}'] = pd.NA\n",
        "\n",
        "    # Get unique UniProt IDs\n",
        "    unique_ids = df['UniProt_ID'].dropna().unique()\n",
        "    print(f\"Processing {len(unique_ids)} unique UniProt IDs...\")\n",
        "\n",
        "    # Process IDs with threading\n",
        "    start_time = time.time()\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = {executor.submit(get_uniprot_features, uid): uid for uid in unique_ids}\n",
        "\n",
        "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Fetching features\"):\n",
        "            pass\n",
        "\n",
        "    # Apply to dataframe\n",
        "    print(\"Applying features to dataframe...\")\n",
        "    for i in range(1, max_sites+1):\n",
        "        df[f'Position_{i}'] = df['UniProt_ID'].map(\n",
        "            lambda x: UNIPROT_CACHE.get(x, [])[i-1][0]\n",
        "            if x in UNIPROT_CACHE and len(UNIPROT_CACHE[x]) >= i else pd.NA\n",
        "        )\n",
        "        df[f'Sequence_{i}'] = df['UniProt_ID'].map(\n",
        "            lambda x: UNIPROT_CACHE.get(x, [])[i-1][1]\n",
        "            if x in UNIPROT_CACHE and len(UNIPROT_CACHE[x]) >= i else pd.NA\n",
        "        )\n",
        "\n",
        "    # CORRECTED COLUMN ORDERING\n",
        "    cols = list(df.columns)\n",
        "    gene_idx = cols.index('GeneSymbol')\n",
        "\n",
        "    # Get properly sorted columns\n",
        "    pos_cols = sorted([c for c in cols if c.startswith('Position_')],\n",
        "                     key=lambda x: int(x.split('_')[1]))\n",
        "    seq_cols = sorted([c for c in cols if c.startswith('Sequence_')],\n",
        "                     key=lambda x: int(x.split('_')[1]))\n",
        "\n",
        "    # Rebuild column order\n",
        "    cols = [c for c in cols if c not in pos_cols + seq_cols]\n",
        "    cols[gene_idx+1:gene_idx+1] = pos_cols + seq_cols\n",
        "\n",
        "    # Save output\n",
        "    df[cols].to_csv(output_file, index=False)\n",
        "\n",
        "    elapsed = (time.time() - start_time)/60\n",
        "    print(f\"\\nProcessed {len(df)} rows in {elapsed:.2f} minutes\")\n",
        "    print(f\"Results saved to {output_file}\")\n",
        "    print(f\"Found binding sites for {len(UNIPROT_CACHE)} proteins\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    process_csv(\n",
        "        input_file=\"variant_summary_with_uniprot_ids_updated_file.csv\",  # Replace with your CSV file\n",
        "        output_file=\"output_with_sites.csv\",\n",
        "        max_sites=20,      # Number of position/sequence pairs to include\n",
        "        max_workers=8      # Number of concurrent requests\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
